{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dropout\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils.np_utils import to_categorical\n",
    "from keras.layers import Dense, Input,  Flatten\n",
    "from keras.layers import Conv1D, MaxPooling1D, AveragePooling1D, Embedding, LSTM\n",
    "\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "import os\n",
    "import csv\n",
    "import numpy as np\n",
    "from numpy.random import RandomState\n",
    "import tensorflow as tf\n",
    "prng = RandomState(1234567890)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "MAX_SEQUENCE_LENGTH = 150\n",
    "MAX_NB_WORDS = 20000\n",
    "\n",
    "BASE_DIR = '.'\n",
    "GLOVE_DIR = BASE_DIR + '/glove/'\n",
    "\n",
    "# Consider changing the 200 to 25\n",
    "EMBEDDING_DIM = 25\n",
    "GLOVE_FILE = 'glove.twitter.27B.25d.txt'\n",
    "\n",
    "TRAIN_DATA_FILE = \"Sentiment Analysis Dataset.csv\"\n",
    "\n",
    "VALIDATION_SPLIT = 0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# consider outsourcing the preprocessing (tokenize + embeding) into a dictionary file)\n",
    "def main():\n",
    "\n",
    "    #os.environ['CUDA_VISIBLE_DEVICES'] = '1'  # str(random.randint(0, 15))\n",
    "    labels_index = { 'Negative': 0, 'Positive': 1}\n",
    "\n",
    "\n",
    "\n",
    "    word_index, x_train, x_val, y_train, y_val = get_training_and_validation_sets()\n",
    "\n",
    "    #with tf.device('/gpu:0'):\n",
    "    model = make_model(labels_index, word_index)\n",
    "    train(model, x_train, x_val, y_train, y_val)\n",
    "\n",
    "    valid_predicted_out = model.predict(x=x_val, batch_size=256)\n",
    "    evaluate(y_val, valid_predicted_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_training_and_validation_sets():\n",
    "    X_raw, Y_raw = load_data_set()\n",
    "    X_processed, Y_processed, word_index = tokenize_data(X_raw, Y_raw)\n",
    "    x_train, x_val, y_train, y_val = split_the_data(X_processed, Y_processed)\n",
    "    return word_index, x_train, x_val, y_train, y_val\n",
    "\n",
    "\n",
    "def train(model, x_train, x_val, y_train, y_val):\n",
    "    print(\"Train\")\n",
    "    cb = [ModelCheckpoint(\"weights.h5\", save_best_only=True, save_weights_only=False)]\n",
    "    model.fit(x_train, y_train, validation_data=(x_val, y_val), epochs=5, batch_size=256, callbacks=cb)\n",
    "    try:\n",
    "        os.remove(\"model.h5\")\n",
    "    except OSError:\n",
    "        pass\n",
    "    model.save(\"model.h5\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def evaluate(expected_out, predicted_out):\n",
    "    expected_categories = [np.argmax(x) for x in expected_out]\n",
    "    predicted_categories = [np.argmax(x) for x in predicted_out]\n",
    "    cm = confusion_matrix(expected_categories, predicted_categories)\n",
    "    print(cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def make_model(labels_index, word_index):\n",
    "    embedded_sequences = make_embedding_layer(word_index)\n",
    "    # Check replacing CNN to RNN with LSTM.\n",
    "    # Check diff activations? softmax->tanh\n",
    "    # Consider adding batch normalization\n",
    "    lstm_out = 196\n",
    "    model = Sequential([\n",
    "        embedded_sequences,\n",
    "        #Conv1D(512, 5, activation='relu'),\n",
    "        #AveragePooling1D(5),\n",
    "        #Conv1D(256, 5, activation='relu'),\n",
    "        #AveragePooling1D(5),\n",
    "        #Conv1D(128, 5, activation='relu'),\n",
    "        #MaxPooling1D(5),\n",
    "        #Flatten(),\n",
    "        #Dropout(0.3),\n",
    "        #Dense(128, activation='relu'),\n",
    "        LSTM(lstm_out, dropout=0.2, recurrent_dropout=0.2),\n",
    "        Dense(len(labels_index), activation='softmax')\n",
    "    ])\n",
    "\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['acc'])\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def make_embedding_layer(word_index):\n",
    "    embeddings = get_embeddings()\n",
    "    nb_words = min(MAX_NB_WORDS, len(word_index))\n",
    "    embedding_matrix = np.zeros((nb_words, EMBEDDING_DIM))\n",
    "\n",
    "    for word, i in word_index.items():\n",
    "        if i >= MAX_NB_WORDS:\n",
    "            continue\n",
    "        embedding_vector = embeddings.get(word)\n",
    "        if embedding_vector is not None:\n",
    "            embedding_matrix[i] = embedding_vector\n",
    "\n",
    "    embedding_layer = Embedding(nb_words, EMBEDDING_DIM, weights=[embedding_matrix], input_length=MAX_SEQUENCE_LENGTH, trainable=False)\n",
    "    return embedding_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def split_the_data(X_processed, Y_processed):\n",
    "    indices = np.arange(X_processed.shape[0])\n",
    "    prng.shuffle(indices)\n",
    "    X_processed = X_processed[indices]\n",
    "    Y_processed = Y_processed[indices]\n",
    "    nb_validation_samples = int(VALIDATION_SPLIT * X_processed.shape[0])\n",
    "    x_train = X_processed[:-nb_validation_samples]\n",
    "    y_train = Y_processed[:-nb_validation_samples]\n",
    "    x_val = X_processed[-nb_validation_samples:]\n",
    "    y_val = Y_processed[-nb_validation_samples:]\n",
    "\n",
    "    return x_train, x_val, y_train, y_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def tokenize_data(X_raw, Y_raw):\n",
    "    tokenizer = Tokenizer(nb_words=MAX_NB_WORDS)\n",
    "    tokenizer.fit_on_texts(X_raw)\n",
    "    sequences = tokenizer.texts_to_sequences(X_raw)\n",
    "    word_index = tokenizer.word_index\n",
    "    X_processed = pad_sequences(sequences, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "    Y_processed = to_categorical(np.asarray(Y_raw), 2)\n",
    "\n",
    "    return X_processed, Y_processed, word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_data_set():\n",
    "    X = []\n",
    "    Y = []\n",
    "    with open(TRAIN_DATA_FILE) as f:\n",
    "        reader = csv.reader(f, delimiter=\",\")\n",
    "        for i, line in enumerate(reader):\n",
    "            is_positive = line[1]==\"1\"\n",
    "            text = line[3]\n",
    "            X.append(text)\n",
    "            Y.append(is_positive)\n",
    "    return X,Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_embeddings():\n",
    "    embeddings = {}\n",
    "    with open(os.path.join(GLOVE_DIR, GLOVE_FILE), 'r') as f:\n",
    "        for line in f:\n",
    "            values = line.split()\n",
    "            word = values[0]\n",
    "            coefs = np.asarray(values[1:], dtype='float32')\n",
    "            embeddings[word] = coefs\n",
    "    return embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bowenshen/anaconda3/lib/python3.6/site-packages/keras/preprocessing/text.py:145: UserWarning: The `nb_words` argument in `Tokenizer` has been renamed `num_words`.\n",
      "  warnings.warn('The `nb_words` argument in `Tokenizer` '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train\n",
      "Train on 1262892 samples, validate on 315723 samples\n",
      "Epoch 1/5\n",
      "1262892/1262892 [==============================] - 1243s - loss: 0.5421 - acc: 0.7208 - val_loss: 0.4955 - val_acc: 0.7554\n",
      "Epoch 2/5\n",
      "1262892/1262892 [==============================] - 1196s - loss: 0.4903 - acc: 0.7601 - val_loss: 0.4558 - val_acc: 0.7825\n",
      "Epoch 3/5\n",
      "1262892/1262892 [==============================] - 1236s - loss: 0.4712 - acc: 0.7727 - val_loss: 0.4404 - val_acc: 0.7928\n",
      "Epoch 4/5\n",
      "1262892/1262892 [==============================] - 1225s - loss: 0.4603 - acc: 0.7799 - val_loss: 0.4300 - val_acc: 0.7994\n",
      "Epoch 5/5\n",
      "1262892/1262892 [==============================] - 1226s - loss: 0.4526 - acc: 0.7850 - val_loss: 0.4266 - val_acc: 0.8007\n",
      "[[121808  35622]\n",
      " [ 27307 130986]]\n"
     ]
    }
   ],
   "source": [
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
